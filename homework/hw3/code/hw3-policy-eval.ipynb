{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eec4d70-88bf-4ce2-b1a8-1e267babf051",
   "metadata": {},
   "source": [
    "## Homework 3: Offline Policy Value Estimation (i.e. Counterfactual evaluation)\n",
    "\n",
    "### Introduction\n",
    "In this lab, we're going to be reproducing a few results from http://proceedings.mlr.press/v97/vlassis19a.html, and extending their results in a few ways.  Here's an overview: We start by taking a multiclass classification problem and splitting it into train and test.  There are 26 classes, which we'll interpret as 26 possible actions to take for every input context. On the training set, we fit a multinomial logistic regression model to predict the correct label/best action.  Following the paper, we create a logging policy based on this model (details supplied in the relevant spot below).  We then generate \"logged bandit feedback\" for this logging policy using the **test** set.  Given this logged bandit feedback, we'll try out several different methods for estimating the value of various policies.  We'll also estimate the value of each of these policies using the full-feedback (i.e. the full observed rewards), and we'll treat that as the ground truth value for the purpose of performance assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9661661-6ece-47fd-b483-bf6481b2f854",
   "metadata": {},
   "source": [
    "### Load and process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5e0fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0791e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_integer_dtype\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "from scipy.special import expit\n",
    "import seaborn as sns\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore');\n",
    "import sys\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22d10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fully_observed_bandit():\n",
    "    \"\"\"\n",
    "    This loads in a multiclass classification problem and reformulates it as a fully observed bandit problem.\n",
    "    \n",
    "    \"\"\"\n",
    "    df_l = pd.read_csv('data/letter-recognition.data',\n",
    "                       names = ['a']+[f'x{i}' for i in range(16)])\n",
    "    X = df_l.drop(columns=['a'])\n",
    "\n",
    "    # Convert labels to ints and one-hot\n",
    "    y = df_l['a']\n",
    "    # if y is not column of integers (that represent classes), then convert\n",
    "    if not is_integer_dtype(y.dtype):\n",
    "        y = y.astype('category').cat.codes\n",
    "\n",
    "    ## Full rewards\n",
    "    n = len(y)\n",
    "    k = max(y)+1\n",
    "    full_rewards = np.zeros([n, k])\n",
    "    full_rewards[np.arange(0,n),y] = 1\n",
    "    contexts = X\n",
    "    best_actions = y\n",
    "    return contexts, full_rewards, best_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2afd69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26 actions, the context space is 16 dimensional, and there are 20000 examples.\n",
      "For example, the first item has context vector:\n",
      "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9  x10  x11  x12  x13  x14  x15\n",
      "0   2   8   3   5   1   8  13   0   6   6   10    8    0    8    0    8.\n",
      "The best action is 19.  The reward for that action is 1 and all other actions get reward 0.\n",
      "The reward information is store in full_rewards as the row\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0.].\n"
     ]
    }
   ],
   "source": [
    "contexts, full_rewards, best_actions = get_fully_observed_bandit()\n",
    "n, k = full_rewards.shape\n",
    "_, d = contexts.shape\n",
    "print(f\"There are {k} actions, the context space is {d} dimensional, and there are {n} examples.\")\n",
    "print(f\"For example, the first item has context vector:\\n{contexts.iloc[0:1]}.\")\n",
    "print(f\"The best action is {best_actions[0]}.  The reward for that action is 1 and all other actions get reward 0.\")\n",
    "print(f\"The reward information is store in full_rewards as the row\\n{full_rewards[0]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba6ba4-b43c-4447-9465-35ae762cb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose train/test indices\n",
    "rng = default_rng(7)\n",
    "train_frac = 0.5\n",
    "train_size = round(train_frac * n)\n",
    "train_idx = rng.choice(n, size = train_size, replace = False)\n",
    "test_idx = np.setdiff1d(np.arange(n), train_idx, assume_unique=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fd5649-06c5-4993-8c84-c043586ade18",
   "metadata": {},
   "source": [
    "### Policies\n",
    "In this section, we'll build out a Policy class, some specific policies, and evaluate policies on full-feedback data.\n",
    "\n",
    "**Problem 1.** Complete the Policy class and the UniformActionPolicy classes below. Run the code provided to get an estimate of the value of the uniform action policy using the test set.  Explain why the value you get makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36a4b2d-9440-4c52-b0ec-502b4ce6cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action_distribution(self, X):\n",
    "        \"\"\"   \n",
    "        This method is intended to be overridden by each implementation of Policy.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts\n",
    "\n",
    "        Returns:\n",
    "            2-dim numpy array with the same number of rows as X and self.num_actions columns. \n",
    "                Each rows gives the policy's probability distribution over actions conditioned on the context in the corresponding row of X\n",
    "        \"\"\"   \n",
    "        raise NotImplementedError(\"Must override method\")\n",
    "\n",
    "    def get_action_propensities(self, X, actions):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions\n",
    "            actions (np.array): actions taken, represented by integers, corresponding to rows of X\n",
    "\n",
    "        Returns:\n",
    "            1-dim numpy array of probabilities (same size as actions) for taking each action in its corresponding context\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        pass\n",
    "\n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions and propensities returned\n",
    "\n",
    "        Returns:\n",
    "            actions (np.array): 1-dim numpy array of length equal to the number of rows of X.  Each entry is an integer indicating the action selected for the corresponding context in X. \n",
    "                The action is selected randomly according to the policy, conditional on the context specified in the appropriate row of X.\n",
    "            propensities (np.array): 1-dim numpy array of length equal to the number of rows of X; gives the propensity for each action selected in actions\n",
    "\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        pass\n",
    "        \n",
    "    def get_value_estimate(self, X, full_rewards):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of full_rewards\n",
    "            full_rewards (np.array): 2-dim numpy array with the same number of rows as X and self.num_actions columns; \n",
    "                each row gives the rewards that would be received for each action for the context in the corresponding row of X.\n",
    "                This would only be known in a full-feedback bandit, or estimated in a direct method\n",
    "\n",
    "        Returns:\n",
    "            scalar value giving the expected average reward received for playing the policy for contexts X and the given full_rewards\n",
    "\n",
    "        \"\"\"   \n",
    "        ## TODO\n",
    "        pass\n",
    "\n",
    "\n",
    "class UniformActionPolicy(Policy):\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6287a4e1-c413-4694-88ba-c086799496da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = contexts.iloc[train_idx].to_numpy()\n",
    "y_train = best_actions.iloc[train_idx].to_numpy()\n",
    "X_test = contexts.iloc[test_idx].to_numpy()\n",
    "y_test = best_actions.iloc[test_idx].to_numpy()\n",
    "full_rewards_test = full_rewards[test_idx]\n",
    "\n",
    "uniform_policy = UniformActionPolicy(num_actions=k)\n",
    "uniform_policy_value = uniform_policy.get_value_estimate(X=X_test, full_rewards=full_rewards_test)\n",
    "print(f\"The estimate of the value of the uniform action policy using the full-feedback test set is {uniform_policy_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c85bfc2-b9e3-4704-ac2c-24d7f8e5d7d7",
   "metadata": {},
   "source": [
    "**Problem 2.**  Complete the SKLearnPolicy class below and run the code that creates two policies and estimates their values using the full reward information in the test set.  You should find that the deterministic policy has a higher value than the stochastic policy.  Nevertheless, why might one choose to deploy the stochastic policy rather than the deterministic policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0a7c0-8186-47e3-bc59-d838c30b9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SKLearnPolicy(Policy):\n",
    "    \"\"\" \n",
    "    An SKLearnPolicy uses a scikit learn model to generate an action distribution.  If the SKLearnPolicy is built with is_deterministic=False, \n",
    "    then the action distribution for a context x should be whatever predict_proba for the model returns.  If is_deterministic=True, then all the probability mass \n",
    "    should be concentrated on whatever predict of the model returns.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_actions=2, is_deterministic=False):\n",
    "        self.is_deterministic = is_deterministic\n",
    "        self.num_actions = num_actions\n",
    "        self.model = model\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## TODO\n",
    "        if (self.is_deterministic):\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        pass\n",
    "\n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        \"\"\" You don't technically have to override this function.  If you just delete this function, the parent class Policy can handle it in a generic way\n",
    "        However, if is_deterministic=True, then the action distribution for each context is trivial -- it always puts probability one for a \n",
    "        particular action and 0 for the others. And so \"randomly\" selecting an action according to this distribution using the code you write\n",
    "        for select_actions in the parent class (Policy) is very inefficient.  You can just use model.predict to get the actions that will be \n",
    "        selected for each context.  That's the idea of the if statement.\"\"\"\n",
    "        ## TODO\n",
    "        if (self.is_deterministic):\n",
    "            pass \n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        pass\n",
    "\n",
    "model = LogisticRegression(multi_class='multinomial')\n",
    "model.fit(X_train, y_train)\n",
    "policy_stochastic = SKLearnPolicy(model=model, num_actions=k, is_deterministic=False)\n",
    "policy_deterministic = SKLearnPolicy(model=model, num_actions=k, is_deterministic=True)\n",
    "\n",
    "policy_stochastic_true_value = policy_stochastic.get_value_estimate(X_test, full_rewards_test)\n",
    "policy_deterministic_true_value = policy_deterministic.get_value_estimate(X_test, full_rewards_test)\n",
    "print(f\"Stochastic policy true value {policy_stochastic_true_value}.\")\n",
    "print(f\"Deterministic policy true value {policy_deterministic_true_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b30e36f-4138-4c30-af2a-1e62f5dd3deb",
   "metadata": {},
   "source": [
    "**Problem 3.** Fill in the VlassisLoggingPolicy class below, and evaluate the value of this logging policy using the code provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041029e4-da4d-498f-90d8-aa53e2350578",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VlassisLoggingPolicy(Policy):\n",
    "    \"\"\"\n",
    "    This policy derives from another deterministic policy following the recipe described in the Vlassis et al paper, on the top of the second column in section 5.3.\n",
    "    For any context x, if the deterministic policy selects action a, then this policy selects action a with probability eps (a supplied parameter), and spreads the\n",
    "    rest of the probability mass uniformly over the other actions.\n",
    "    \"\"\"\n",
    "    def __init__(self, deterministic_target_policy, num_actions=2, eps=0.05,):\n",
    "        self.num_actions = num_actions\n",
    "        self.target_policy = deterministic_target_policy\n",
    "        self.eps = eps\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        ## TODO\n",
    "        pass\n",
    "    \n",
    "logging_policy = VlassisLoggingPolicy(policy_deterministic, num_actions=k, eps=0.05)\n",
    "logging_policy_value = logging_policy.get_value_estimate(X=X_test, full_rewards=full_rewards_test)\n",
    "print(f\"The estimate of the value of the logging policy using the full-feedback test set is {logging_policy_value}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d606843e-58ec-481f-a04b-d8deced26f4e",
   "metadata": {},
   "source": [
    "### Simulate bandit feedback and on-policy evaluation\n",
    "**Problem 4.** Take a look at the generate_bandit_feedback function, so you understand how it works.  Then generate bandit feedback using the test data -- generate as many rounds are there are contexts in the test data. Use the result to generate an \"on-policy\" estimate of the value of the logging policy.  How does it compare to our \"ground truth\" estimate you found previously using the full-feedback test set? Repeat using 1/100th, 1/10th, and 10x as much bandit feedback, to see how much the value estimates change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb89039-c4f1-4ff7-9957-d6fded9a7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bandit_feedback(contexts, full_rewards, policy,\n",
    "                             new_n = None,\n",
    "                             rng=default_rng(1)):\n",
    "    \"\"\"   \n",
    "    Args:\n",
    "        contexts (np.array): contexts, rows correspond to entries of rewards\n",
    "        full_rewards (np.array): 2-dim numpy array with the same number of rows as X and number of columns corresponding to the number actions\n",
    "            each row gives the reward that would be received for each action for the context in the corresponding row of X. \n",
    "\n",
    "    Returns:\n",
    "        new_contexts (np.array): new_n rows and same number of columns as in contexts\n",
    "        actions (np.array): vector with new_n entries giving actions selected by the provided policy for the contexts in new_contexts\n",
    "        observed_rewards (np.array): vector with new_n entries giving rewards received for the actions taken (in actions) in each context of new_contexts \n",
    "    \"\"\"   \n",
    "    \n",
    "    if new_n is None:\n",
    "        new_n = contexts.shape[0]\n",
    "    n, k = full_rewards.shape\n",
    "    num_repeats = np.ceil(new_n / n).astype(int)\n",
    "    new_contexts = np.tile(contexts, [num_repeats,1])\n",
    "    new_contexts = new_contexts[0:new_n]\n",
    "    new_rewards = np.tile(full_rewards, [num_repeats,1])\n",
    "    new_rewards = new_rewards[0:new_n]\n",
    "    actions, propensities = policy.select_actions(X=new_contexts, rng=rng)\n",
    "    observed_rewards = new_rewards[np.arange(new_n), actions]\n",
    "    return new_contexts, actions, observed_rewards, propensities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6829a2bc-4cad-4b46-be89-ea68450c3c65",
   "metadata": {},
   "source": [
    "### Test out off-policy value estimators\n",
    "**Problem 5.** Complete the get_value_estimators function below, per the specification.  Include the following estimators\n",
    "- Unweighted mean (done for you)\n",
    "- Importance-weighted (IW) value estimator\n",
    "- Self-normalized IW mean\n",
    "- Direct method with linear ridge regression reward predictor fit for each action\n",
    "- Direct method with IW-linear ridge regression reward predictor fit for each action\n",
    "- [Optional (not for credit)] Direct method with a non-linear reward predictor fit for each action\n",
    "- [Optional (not for credit)] Direct method with a non-linear reward predictor fit for all actions at once (action becomes part of the input)\n",
    "\n",
    "Run the code below that will apply your value estimators to a policy on logged bandit feedback. Verify that your results are reasonable. (Don't worry if your numbers are not a very close match for the results in the table.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6eeeb-6f2e-47c9-af9a-91d123f824d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build our value estimators\n",
    "\n",
    "def get_value_estimators(policy, contexts, actions, rewards, propensities, skip_slow_stuff=False):\n",
    "    \"\"\"   \n",
    "    Args:\n",
    "        policy (Policy): the policy we want to get a value estimate for\n",
    "        contexts (np.array): contexts from bandit feedback\n",
    "        actions (np.array): actions chosen for bandit feedback\n",
    "        rewards (np.array): rewards received in bandit feedback\n",
    "        propensities (np.array): the propensity for each action selected under the logging policy (which is not provided to this function)\n",
    "        skip_slow_stuff (boolean): boolean flag which allows you to turn on/off some slow estimators (ignore this if you like)\n",
    "    Returns:\n",
    "        est (dict): keys are string describing the value estimator, values are the corresponding value estimates \n",
    "    \"\"\"   \n",
    "\n",
    "    est = {}\n",
    "    est[\"mean\"] = np.mean(rewards)\n",
    "    ## TODO\n",
    "\n",
    "    return est\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa3f6c-b2e5-46c0-90af-e27953efa13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator_stats(estimates, true_parameter_value=None):\n",
    "    \"\"\"\n",
    " \n",
    "     Args:\n",
    "        estimates (pd.DataFrame): each row corresponds to collection of estimates for a sample and\n",
    "            each column corresponds to an estimator\n",
    "        true_parameter_value (float): the true parameter value that we will be comparing estimates to\n",
    "            \n",
    "    Returns:\n",
    "        pd.Dataframe where each row represents data about a single estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    est_stat = []\n",
    "    for est in estimates.columns:\n",
    "        pred_means = estimates[est]\n",
    "        stat = {}\n",
    "        stat['stat'] = est\n",
    "        stat['mean'] = np.mean(pred_means)\n",
    "        stat['SD'] = np.std(pred_means)\n",
    "        stat['SE'] = np.std(pred_means) / np.sqrt(len(pred_means))\n",
    "        if true_parameter_value:\n",
    "            stat['bias'] = stat['mean'] - true_parameter_value\n",
    "            stat['RMSE'] = np.sqrt(np.mean((pred_means - true_parameter_value) ** 2))\n",
    "        est_stat.append(stat)\n",
    "\n",
    "    return pd.DataFrame(est_stat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b7592-3e73-4813-9012-405244df920a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts_test, actions_test, rewards_test, propensities_test = generate_bandit_feedback(contexts=X_test, full_rewards=full_rewards_test, policy=logging_policy, rng=default_rng(6))\n",
    "policy = policy_deterministic\n",
    "est = get_value_estimators(policy, contexts_test, actions_test, rewards_test, propensities_test)\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "print(f\"policy true value {policy_true_value}.\")\n",
    "df = pd.DataFrame(est, index=[0])\n",
    "est"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e1a74f-92ff-46cf-b478-2f25e95078fa",
   "metadata": {},
   "source": [
    "**Problem 6.** Run the code below to test your value estimators across multiple trials.  Write a few sentences about anything you learned from these experiments or that you find interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad590855",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=20\n",
    "val_ests = []\n",
    "policy = policy_deterministic\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "rng=default_rng(6)\n",
    "for i in range(trials):\n",
    "    contexts, actions, rewards, propensities = generate_bandit_feedback(X_test, full_rewards_test, logging_policy, rng=rng)\n",
    "    est = get_value_estimators(policy, contexts, actions, rewards, propensities)\n",
    "    val_ests.append(est)\n",
    "\n",
    "df = pd.DataFrame(val_ests)\n",
    "print(get_estimator_stats(df, true_parameter_value=policy_true_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f034813-be82-491a-9f1d-7337f24b0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=20\n",
    "val_ests = []\n",
    "policy = policy_stochastic\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "rng=default_rng(6)\n",
    "for i in range(trials):\n",
    "    contexts, actions, rewards, propensities = generate_bandit_feedback(X_test, full_rewards_test, logging_policy, rng=rng)\n",
    "    est = get_value_estimators(policy, contexts, actions, rewards, propensities)\n",
    "    val_ests.append(est)\n",
    "\n",
    "df = pd.DataFrame(val_ests)\n",
    "print(get_estimator_stats(df, true_parameter_value=policy_true_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b8a89f-fe78-4fe5-80a6-37026e360193",
   "metadata": {},
   "outputs": [],
   "source": [
    "trials=20\n",
    "val_ests = []\n",
    "policy = uniform_policy\n",
    "policy_true_value = policy.get_value_estimate(X_test, full_rewards_test)\n",
    "rng=default_rng(6)\n",
    "for i in range(trials):\n",
    "    contexts, actions, rewards, propensities = generate_bandit_feedback(X_test, full_rewards_test, logging_policy, rng=rng)\n",
    "    est = get_value_estimators(policy, contexts, actions, rewards, propensities)\n",
    "    val_ests.append(est)\n",
    "\n",
    "df = pd.DataFrame(val_ests)\n",
    "print(get_estimator_stats(df, true_parameter_value=policy_true_value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
