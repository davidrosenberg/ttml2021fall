{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: ATE Estimation with voter turnout data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we'll be working with some of the data described in the paper [“Social Pressure and Voter Turnout: Evidence from a Large-Scale Field Experiment”](http://isps.yale.edu/sites/default/files/publication/2012/12/ISPS08-001.pdf) by Gerber et al. (2008).  We attained the data from this [Github repo](https://github.com/gsbDBI/ExperimentData/tree/master/Social), specifically this [file](https://raw.githubusercontent.com/gsbDBI/ExperimentData/master/Social/ProcessedData/socialpresswgeooneperhh_NEIGH.csv). It's also included in the assignment zip file.  In this assignment we'll build several ATE estimators by reducing to two estimations from partially observed data, as discussed in lecture. Parts of this notebook are based on a [notebook](https://gsbdbi.github.io/ml_tutorial/ate_tutorial/ate_tutorial.html) from a tutorial/course on causal inference at Stanford GSB.  Although not necessary, you may find it interesting to refer to that notebook, as they give more details about the covariates, and they they cover some methods that we don't get into (and vice versa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from scipy.stats import norm, sem\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from sklearn.calibration import calibration_curve\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats.stats import pearsonr\n",
    "from numpy.random import default_rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYCddcESYhE6"
   },
   "outputs": [],
   "source": [
    "## Read the data, select some columns, and lightly process\n",
    "df = pd.read_csv('sp.csv.xz')\n",
    "cts_variables_names = [\"yob\", \"hh_size\", \"totalpopulation_estimate\",\n",
    "                         \"percent_male\", \"median_age\",\n",
    "                         \"percent_62yearsandover\",\n",
    "                         \"percent_white\", \"percent_black\",\n",
    "                         \"percent_asian\", \"median_income\",\n",
    "                         \"employ_20to64\", \"highschool\", \"bach_orhigher\",\n",
    "                         \"percent_hispanicorlatino\"]\n",
    "binary_variables_names = [\"sex\",\"g2000\", \"g2002\", \"p2000\", \"p2002\", \"p2004\"]\n",
    "scaled_cts_covariates = StandardScaler().fit_transform(df[cts_variables_names])\n",
    "binary_covariates = df[binary_variables_names]\n",
    "d = pd.DataFrame(np.concatenate((scaled_cts_covariates, binary_covariates), axis=1), \n",
    "                        columns=cts_variables_names+binary_variables_names, index=df.index)\n",
    "d[\"W\"] = df[\"treat_neighbors\"]\n",
    "d[\"Y\"] = df[\"outcome_voted\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: ATE for RCT\n",
    "All individuals in this experiment had an equal probability of being assigned to the treatment group, so the difference-of-means will be a reasonable estimator for the average treatment effect (ATE).  Write a function that computes the difference-of-means estimator for the treatment effect, along with an approximate 95% confidence interval.  Apply it to the dataset d computed above and report the results.  Save the estimate of the ATE and the radius of the confidence interval for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_of_means(d, alpha=0.05):\n",
    "    # TODO\n",
    "\n",
    "ate, ate_CI_radius = get_diff_of_means(d, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: ATE estimation with known confounders\n",
    "In this problem we're going to take a relatively small and biased subsample of our full dataset and try to use that to estimate the ATE. Our approach is to reduce ATE estimation to estimating a mean in the MAR setting.\n",
    "\n",
    "Below we give a function that takes a biased sample using an approach similar to [this one](https://gsbdbi.github.io/ml_tutorial/ate_tutorial/ate_tutorial.html#introducing_sampling_bias). The details aren't important for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likely_voters =  (((d['g2002'] == 1) & (d['sex'] == 0)) | (d['p2004'] == 1))\n",
    "unlikely_voters_control = (~likely_voters) & (d[\"W\"] == 0)\n",
    "likely_voters_treatment = likely_voters & (d[\"W\"] == 1)\n",
    "\n",
    "def get_biased_sample(d, overall_subsample_rate=.03, bias_rate=.4, rng=default_rng(0)):\n",
    "    keep_prob = overall_subsample_rate * np.ones(len(d))\n",
    "    keep_prob[unlikely_voters_control] *= bias_rate\n",
    "    keep_prob[likely_voters_treatment] *= bias_rate\n",
    "    keep = rng.random(len(d)) <= keep_prob\n",
    "    d_bias = d[keep]\n",
    "    return d_bias\n",
    "\n",
    "d_bias = get_biased_sample(d)\n",
    "print(f\"We've sampled {len(d_bias)} instances out of {len(d)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Below we provide the function get_basic_MAR_estimators, which computes the complete-case mean and the IPW mean in the MAR setting with known propensity scores. You are to complete the get_ATE_estimators function below so that it *uses the get_basic_MAR_estimators function* and computes one ATE estimate for each type of estimator produced by get_basic_MAR_estimators (i.e. the complete-case mean and the IPW mean).  Use logistic regression to estimate the propensity scores from the data provided to the function.  Run the existing code block below to display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_MAR_estimators(Y, X, pi, R):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        Y (pd.Series): the measurement of the outcome Y_i\n",
    "        X (pd.DataFrame): covariates, rows correspond to entries of Y\n",
    "        R (pd.Series): boolean series indicating whether Y was observed\n",
    "        pi (pd.Series): propensity scores corresponding to observations\n",
    "    \n",
    "    Returns:\n",
    "        dict of estimator names and estimates for EY\n",
    "    \"\"\"\n",
    "    est = {}\n",
    "    n = len(Y)\n",
    "    ## All the estimators below assume we know the pi (i.e. \"missing by design\")\n",
    "    est[\"mean\"] = np.mean(Y[R])\n",
    "    est[\"ipw_mean\"] = np.sum(Y[R] / pi[R]) / n\n",
    "    return est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ATE_estimators(Y, X, W, get_MAR_estimators=get_basic_MAR_estimators):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        Y (pd.Series): the measurement of the outcome Y_i\n",
    "        X (pd.DataFrame): covariates, rows correspond to entries or Y\n",
    "        W (pd.Series): 0/1 series indicating control (0) or treatment (1) assignment\n",
    "        get_MAR_estimators: function behaving like get_basic_MAR_estimators above\n",
    "    \n",
    "    Returns:\n",
    "        dict of ATE estimator names and estimates, same format as for get_MAR_estimators\n",
    "    \"\"\"   \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the following and report \n",
    "d_bias = get_biased_sample(d, overall_subsample_rate=.03, bias_rate=.4, rng=default_rng(0))\n",
    "X = d_bias.drop(columns=['W','Y'])\n",
    "ate_est = get_ATE_estimators(Y=d_bias[\"Y\"], X=X, W=d_bias[\"W\"], get_MAR_estimators=get_basic_MAR_estimators)\n",
    "print(ate_est)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "In this part we significantly expand our ATE estimators and see how they perform over repeated trials.  Complete get_MAR_estimators below to include \n",
    "- Complete-case mean (done for you)\n",
    "- IPW mean (done for you)\n",
    "- Self-normalized IPW mean\n",
    "- Linear regression imputation\n",
    "- IPW linear regression imputation (as defined in lecture)\n",
    "- IW linear regression imputation (as defined in lecture)\n",
    "- Augmented IPW using linear regression\n",
    "- [Optional (not for credit): try using an optimal beta in AIPW, nonlinear regression, or any other variations you'd like to try]\n",
    "\n",
    "Run the code below to assess performance of these estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MAR_estimators(Y, X, pi, R):\n",
    "    \"\"\"\n",
    "    \n",
    "    Args:\n",
    "        Y (pd.Series): the measurement of the outcome Y_i\n",
    "        X (pd.DataFrame): covariates, rows correspond to entries or Y\n",
    "        R (pd.Series): boolean series indicating whether Y was observed\n",
    "        pi (pd.Series): propensity scores corresponding to observations\n",
    "    \n",
    "    Returns:\n",
    "        dict of estimator names and estimates for EY\n",
    "    \"\"\"\n",
    "    est = {}\n",
    "    n = len(Y)\n",
    "    ## All the estimators below assume we know the pi (i.e. \"missing by design\")\n",
    "    est[\"mean\"] = np.mean(Y[R])\n",
    "    est[\"ipw_mean\"] = np.sum(Y[R] / pi[R]) / n\n",
    "    \n",
    "    ## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator_stats(estimates, true_parameter_value=None):\n",
    "    \"\"\"\n",
    " \n",
    "     Args:\n",
    "        estimates (pd.DataFrame): each row corresponds to collection of estimates for a sample and\n",
    "            each column corresponds to an estimator\n",
    "        true_parameter_value (float): the true parameter value that we will be comparing estimates to\n",
    "            \n",
    "    Returns:\n",
    "        pd.Dataframe where each row represents data about a single estimator\n",
    "    \"\"\"\n",
    "    \n",
    "    est_stat = []\n",
    "    for est in estimates.columns:\n",
    "        pred_means = estimates[est]\n",
    "        stat = {}\n",
    "        stat['stat'] = est\n",
    "        stat['mean'] = np.mean(pred_means)\n",
    "        stat['SD'] = np.std(pred_means)\n",
    "        stat['SE'] = np.std(pred_means) / np.sqrt(len(pred_means))\n",
    "        if true_parameter_value:\n",
    "            stat['bias'] = stat['mean'] - true_parameter_value\n",
    "            stat['RMSE'] = np.sqrt(np.mean((pred_means - true_parameter_value) ** 2))\n",
    "        est_stat.append(stat)\n",
    "\n",
    "    return pd.DataFrame(est_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(sampler, num_repeats=10,rng=default_rng(0)):\n",
    "    data_list = []\n",
    "    num_obs_list = []\n",
    "    for i in range(num_repeats): \n",
    "        d = sampler(rng)\n",
    "        X = d.drop(columns=['W','Y'])\n",
    "        ate_est = get_ATE_estimators(Y=d[\"Y\"], X=X, W=d[\"W\"], get_MAR_estimators=get_MAR_estimators)\n",
    "        data_list.append(ate_est)\n",
    "    results = pd.DataFrame(data_list)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampler(rng):\n",
    "    return get_biased_sample(d, overall_subsample_rate=.06, bias_rate=.4, rng=rng)\n",
    "rng = default_rng(0)\n",
    "results = run_experiments(sampler, num_repeats=500, rng=rng) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_eval = get_estimator_stats(results, true_parameter_value=ate)\n",
    "results_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C\n",
    "You should see that the ATE estimate based on the IPW mean is significantly biased, and the bias seems to be driving most of the RMSE. In class we showed that the IPW mean is an unbiased estimator for EY in the MAR setting, and the corresponding ATE estimator is also unbiased.  Why are we seeing bias in this experiment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Bootstrap confidence intervals for ATE\n",
    "Let's now consider a more realistic scenario, in which we only have a single sample to work with.  In this case it's very important to be able to include some form of uncertainty measure with our estimates.  In this problem we'll do this using the normal approximated bootstrap confidence intervals described in our module on CATEs. However, in this setting, rather than estimating the CATE, we're just estimating the ATE. \n",
    "\n",
    "#### Part A\n",
    "Complete the function get_stratified_bootstrap_CI to generate 95% normal approximated bootstrap confidence intervals for each of the ATE estimators we've developed above. Execute the code below to test the function and produce the results as a table and in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bootstrap_stats(boot_estimates, full_estimates, alpha=0.05):\n",
    "    est_stat = []\n",
    "    signif_level = -norm.ppf(alpha/2)\n",
    "    for est in full_estimates:\n",
    "        est_boot = np.array(boot_estimates[est])\n",
    "        stat = {}\n",
    "        stat['estimator'] = est\n",
    "        stat['estimate'] = full_estimates[est]\n",
    "        #stat['mean_boot'] = np.mean(est_boot)\n",
    "        stat['SD'] = np.std(est_boot)\n",
    "        stat['CI_radius'] = signif_level * stat['SD']\n",
    "        stat['lower_ci'] = stat['estimate'] - stat['CI_radius']\n",
    "        stat['upper_ci'] = stat['estimate'] + stat['CI_radius']\n",
    "        est_stat.append(stat)\n",
    "\n",
    "    return pd.DataFrame(est_stat)\n",
    "\n",
    "\n",
    "def get_stratified_bootstrap_CI(Y, X, W, estimators, num_bootstrap=10, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        pd.Dataframe with\n",
    "            column \"estimator\" with the name of the estimator,\n",
    "            column \"estimate\" which is the estimate based on (Y,X,W)\n",
    "                which serves as the center of our bootstrap confidence intervals\n",
    "            column \"CI_radius\" which is the radius of the confidence interval\n",
    "            (additional columns may be included as you wish)\n",
    "    \"\"\"    \n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = default_rng(10)\n",
    "d_samp = sampler(rng) # our single biased sample\n",
    "X = d_samp.drop(columns=['W','Y'])\n",
    "estimators = lambda Y, X, W: get_ATE_estimators(Y, X, W, get_MAR_estimators=get_MAR_estimators)\n",
    "ci = get_stratified_bootstrap_CI(Y=d_samp[\"Y\"], X=X, W=d_samp[\"W\"], estimators=estimators, num_bootstrap=500)\n",
    "print(f\"A 95% confidence interval for the ATE based on the full dataset is {ate}+/-{ate_CI_radius}\")\n",
    "print(ci)\n",
    "to_plot = ci[['estimator','estimate','CI_radius']]\n",
    "to_plot = to_plot.append({'estimator':'Full data estimate','estimate':ate,'CI_radius':ate_CI_radius}, ignore_index=True)\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,7))\n",
    "ax.errorbar(to_plot['estimate'], np.arange(len(to_plot)), \\\n",
    "            xerr=to_plot['CI_radius'], \n",
    "            fmt='o', elinewidth=3, capsize=5)\n",
    "ax.grid('on')\n",
    "ax.set_yticks(np.arange(len(to_plot)))\n",
    "ax.set_yticklabels(to_plot[\"estimator\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "In the previous part, you should have found that the ATE estimate based on the full data is contained in the confidence intervals for all the estimators from the biased data, with the exception of the complete case mean, which doesn't make any correction for the biased treatment assignment. (The ipw_mean confidence interval is also a bit borderline for some random seeds.) If you look at the chart of all the confidence intervals for a while, you'll notice that the variation in the estimates across most of the estimators is rather small, much smaller than the size of the confidence intervals themselves.  Does this suggest that there's an issue with our approach to generating confidence intervals, or does this seem like a reasonable outcome?  Explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
