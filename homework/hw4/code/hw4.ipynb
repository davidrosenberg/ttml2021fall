{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4: Policy learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "import abc\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "First we define the relevant data and policy objects and load a dataset based on the UCI Letter classification data.\n",
    "\n",
    "Then the assignment has 4 parts:\n",
    "\n",
    "    1. Learning a policy by learning a linear reward model (direct method)\n",
    "    2. Learning a policy by learning a log-linear policy model (IW, clipping, POEM)\n",
    "    3. Considering the effects of model size with neural network models\n",
    "    4. Comparing the various algorithms on different datasets\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: Since we are using a classification dataset and converting it to a simulated bandit problem, we have access to the full reward vectors. We will split the data into train, val, and test sets. The train and val sets will not have access to the full rewards, but to reduce variance of the evaluation we will allow access to the full rewards on the test set. Note this is something we can do in this semi-synthetic data setting, but not something that can usually be done in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 2: This assignment uses pytorch throughout. We have tried to avoid requiring you to fill in the more complicated parts (defining the models and training loops). But, if you are unfamiliar with pytorch it is worth going through this brief tutorial (https://pytorch.org/tutorials/beginner/basics/intro.html) and taking a look at the docs for any specific questions (https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Objects\n",
    "\n",
    "The below code defines the objects that we need for the rest of the assignment to handle data and policies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditDataset:\n",
    "    \"\"\"\n",
    "        Dataset object to simulate offline bandit feedback from \n",
    "        a classification dataset (X,y) and logging policy\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, n_actions, logging_policy, \n",
    "                 seed=0):\n",
    "        self.rng = default_rng(seed)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_actions = n_actions\n",
    "        self.n_data = len(y)\n",
    "        self.logging_policy = logging_policy\n",
    "        \n",
    "        # compute rewards\n",
    "        self.full_rewards = 0.1 * np.ones((self.n_data, self.n_actions))\n",
    "        for i in range(self.n_data):\n",
    "            self.full_rewards[i,self.y[i]] = 0.9\n",
    "        \n",
    "        # sample actions from logging policy\n",
    "        self.actions, self.probs = self.logging_policy.select_actions(self.X, self.rng)\n",
    "            \n",
    "        # compute rewards\n",
    "        self.rewards = self.full_rewards[np.arange(self.n_data), self.actions]\n",
    "        \n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch_size (int): size of batch to be sampled\n",
    "        Returns:\n",
    "            tuple of (contexts, actions, rewards, probs) of shapes\n",
    "                    contexts: (batch_size, context_dim)\n",
    "                    actions: (batch_size, )\n",
    "                    rewards: (batch_size, )\n",
    "                    probs: (batch_size, )\n",
    "        \"\"\"\n",
    "        idx = self.rng.choice(self.n_data, size=batch_size)\n",
    "        contexts = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        actions = torch.tensor(self.actions[idx], dtype=torch.int64)\n",
    "        rewards = torch.tensor(self.rewards[idx], dtype=torch.float32)\n",
    "        probs = torch.tensor(self.probs[idx], dtype=torch.float32)\n",
    "        return contexts, actions, rewards, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(logging_name, train_size=2000):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            logging_name (str): name of the logging policy to be used\n",
    "            train (bool): whether this is the train or test set\n",
    "        Returns:\n",
    "            BanditDataset based on UCI Letter data with the relevant logging policy \n",
    "    \"\"\"\n",
    "    assert train_size < 10001\n",
    "    \n",
    "    # load classification data\n",
    "    df = pd.read_csv('data/letter-recognition.data', header=None)\n",
    "    y = np.array([ord(l) - 65 for l in df[0]])\n",
    "    X = np.array(df[list(range(1,17))])\n",
    "    \n",
    "    # split data\n",
    "    X_train = X[:train_size]\n",
    "    y_train = y[:train_size]\n",
    "    X_val = X[-10000:-9000]\n",
    "    y_val = y[-10000:-9000]\n",
    "    X_test = X[-9000:]\n",
    "    y_test = y[-9000:]\n",
    "\n",
    "    # load policy\n",
    "    if logging_name == 'logistic':\n",
    "        lr = LogisticRegression(multi_class='multinomial', solver='sag', max_iter=10)\n",
    "        n = 1000\n",
    "        lr.fit(X_train[:n], y_train[:n])\n",
    "        policy = PolicyModelPolicy(lr, 26)\n",
    "    elif logging_name == 'uniform':\n",
    "        policy = UniformPolicy(26)\n",
    "    else:\n",
    "        raise NotImplemented\n",
    "        \n",
    "    # create bandit datasets\n",
    "    train_data = BanditDataset(X_train, y_train, 26, policy)\n",
    "    val_data = BanditDataset(X_val, y_val, 26, policy)\n",
    "    test_data = BanditDataset(X_test, y_test, 26, policy)\n",
    "        \n",
    "    # remove access to labels for train and val\n",
    "    train_data.full_reward = None\n",
    "    train_data.y = None\n",
    "    val_data.full_reward = None\n",
    "    val_data.y = None\n",
    "        \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_action_distribution(self, X):\n",
    "        \"\"\"   \n",
    "        This method is intended to be overridden by each implementation of Policy.\n",
    "\n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts\n",
    "\n",
    "        Returns:\n",
    "            2-dim numpy array with the same number of rows as X and self.num_actions columns. \n",
    "                Each rows gives the policy's probability distribution over actions conditioned on the context in the corresponding row of X\n",
    "        \"\"\"   \n",
    "        raise NotImplementedError(\"Must override method\")\n",
    "\n",
    "    def get_action_propensities(self, X, actions):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions\n",
    "            actions (np.array): actions taken, represented by integers, corresponding to rows of X\n",
    "\n",
    "        Returns:\n",
    "            1-dim numpy array of probabilities (same size as actions) for taking each action in its corresponding context\n",
    "        \"\"\"   \n",
    "        dist = self.get_action_distribution(X)\n",
    "        n = X.shape[0]\n",
    "        action_probs = dist[np.arange(n), actions ]\n",
    "        return action_probs\n",
    "\n",
    "    def select_actions(self, X, rng=default_rng(1)):\n",
    "        \"\"\"   \n",
    "        Args:\n",
    "            X (pd.DataFrame): contexts, rows correspond to entries of actions and propensities returned\n",
    "\n",
    "        Returns:\n",
    "            actions (np.array): 1-dim numpy array of length equal to the number of rows of X.  Each entry is an integer indicating the action selected for the corresponding context in X. \n",
    "                The action is selected randomly according to the policy, conditional on the context specified in the appropriate row of X.\n",
    "            propensities (np.array): 1-dim numpy array of length equal to the number of rows of X; gives the propensity for each action selected in actions\n",
    "\n",
    "        \"\"\"   \n",
    "        dist = self.get_action_distribution(X)\n",
    "        n = X.shape[0]\n",
    "        cdf = np.cumsum(dist, axis=1)\n",
    "        tiled = np.tile(rng.random([n,1]), [1, self.num_actions])\n",
    "        actions = np.argmax(tiled <= cdf, axis=1)\n",
    "        propensities = dist[np.arange(n), actions]\n",
    "        return actions, propensities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformPolicy(Policy):\n",
    "    \"\"\"\n",
    "        Uniformly random policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions=2):\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def get_action_distribution(self, X):\n",
    "        action_distribution = np.full([X.shape[0], self.num_actions], 1/self.num_actions)\n",
    "        return action_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyModelPolicy(Policy):\n",
    "    \"\"\"\n",
    "        Policy based on a policy_model that has a predict_proba method\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, policy_model, num_actions, deterministic=False):\n",
    "        self.num_actions = num_actions\n",
    "        self.model = policy_model\n",
    "        self.deterministic = deterministic\n",
    "        \n",
    "    def get_action_distribution(self, X):\n",
    "        probs = self.model.predict_proba(X)\n",
    "        \n",
    "        if self.deterministic:\n",
    "            probs = torch.tensor(probs)\n",
    "            probs = nn.functional.one_hot(torch.argmax(probs, dim=1), \n",
    "                                          self.num_actions).detach().numpy()\n",
    "            \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModelPolicy(Policy):\n",
    "    \"\"\"\n",
    "        Policy based on a reward_model that has a predict method\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reward_model, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.model = reward_model\n",
    "        \n",
    "    def get_action_distribution(self, X):\n",
    "        preds = self.model.predict(X)\n",
    "        preds = torch.tensor(preds)\n",
    "        probs = nn.functional.one_hot(torch.argmax(preds, dim=1), \n",
    "                                          self.num_actions).detach().numpy()\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss):\n",
    "    \"\"\"\n",
    "        Function to plot smoothed learning curves\n",
    "    \"\"\"\n",
    "    plt.plot(smooth(loss))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('step')\n",
    "    plt.show()\n",
    "\n",
    "def smooth(arr, gamma = 0.9):\n",
    "    new_arr = [arr[0]]\n",
    "    for x in arr[1:]:\n",
    "        new_arr.append(gamma * new_arr[-1] + (1-gamma) * x)\n",
    "    return np.array(new_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_full_feedback(policy, dataset):\n",
    "    \"\"\"\n",
    "        Function to evaluate policy on a test dataset that has full_rewards\n",
    "        \n",
    "        Args:\n",
    "            policy (Policy): the policy to be evaluated\n",
    "            dataset (BanditDataset): the dataset to use for evaluation (i.e. the test set)\n",
    "        Returns:\n",
    "            real-valued value estimate\n",
    "    \"\"\"\n",
    "    action_probs = policy.get_action_distribution(dataset.X)\n",
    "    expected_reward_per_context = np.sum(dataset.full_rewards * action_probs, axis=1)\n",
    "    value_est = np.mean(expected_reward_per_context)\n",
    "    \n",
    "    return value_est\n",
    "\n",
    "def evaluate_sniw(policy, dataset):\n",
    "    \"\"\"\n",
    "        Function to evaluate policy on a validation dataset\n",
    "        Uses a self-normalized estimator\n",
    "        \n",
    "        Args:\n",
    "            policy (Policy): the policy to be evaluated\n",
    "            dataset (BanditDataset): the dataset to use for evaluation (i.e. the validation set)\n",
    "        Returns:\n",
    "            real-valued value estimate\n",
    "    \"\"\"\n",
    "    action_probs = policy.get_action_propensities(dataset.X, dataset.actions)\n",
    "    weights = action_probs / dataset.probs\n",
    "\n",
    "    value_est = np.sum(weights * dataset.rewards) / np.sum(weights)\n",
    "    \n",
    "    return value_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 26\n",
    "context_dim = 16\n",
    "\n",
    "log_type = 'logistic'\n",
    "train_size = 2000\n",
    "\n",
    "train_data, val_data, test_data = load_data(log_type, train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: direct method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the reward model\n",
    "\n",
    "First we need a reward model. Here we implement a linear model in pytorch. We could use SKLearn for the linear model, but we will need to use gradient-based optimization later on for policy learning and to use neural models, so instead we will use pytorch throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRewardModel(nn.Module):\n",
    "    def __init__(self, context_dim, n_actions):\n",
    "        super(LinearRewardModel, self).__init__()\n",
    "        self.linear = nn.Linear(context_dim, n_actions)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "            Args: \n",
    "                X (torch.tensor): a tensor of shape (batch_size, context_dim) and dtype torch.float32\n",
    "            Returns:\n",
    "                a torch.tensor of shape (batch_size, n_actions) containing model predictions\n",
    "        \"\"\"\n",
    "        preds = self.linear(X)\n",
    "        return preds\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "            Args: \n",
    "                X (np.ndarray): an array of shape (batch_size, context_dim)\n",
    "            Returns:\n",
    "                np.ndarray of shape (batch_size, n_actions) containing model predictions\n",
    "        \"\"\"\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        return self.forward(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the training loop\n",
    "\n",
    "Since we are using pytorch, we need to define our own training loop. Here we will use the adam optimizer, a variant of SGD (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss_fn, dataset, hyperparams):\n",
    "    \"\"\"\n",
    "        Args: \n",
    "            model (nn.Module): a pytorch model with a forward method that takes contexts\n",
    "            loss_fn (function): a function that takes torch.tensors of (preds, actions, rewards, propensities) \n",
    "                and returns a scalar torch.tensor loss\n",
    "            dataset (BanditDataset): the training set\n",
    "            hyperparams (dict): a dict of hyperparameter values. \n",
    "                    Needs to at least contain:\n",
    "                        n_steps (int): the number of gradient steps to take\n",
    "                        batch_size (int): the batch size for sampling SGD minibatches\n",
    "                        lr (float): the learning rate for the adam optimizer\n",
    "        Returns:\n",
    "            list of training loss on each batch\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=hyperparams['lr'])\n",
    "    \n",
    "    losses = []\n",
    "    for step in tqdm(range(hyperparams['n_steps'])):\n",
    "        contexts, actions, rewards, propensities = dataset.sample_batch(hyperparams['batch_size'])\n",
    "        loss = loss_fn(model(contexts), actions, rewards, propensities, hyperparams)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a: defining the learning objective\n",
    "\n",
    "For this question, please define the loss function for the direct method. The function takes in a batch (preds, actions, rewards, propensities) and returns a scalar tensor for the loss on this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm_loss(preds, actions, rewards, propensities, hyperparams):\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b: learning a policy\n",
    "\n",
    "Now use the training loop to train the LinearRewardModel. Plot the learning curve with plot_loss. Then use evaluate_sniw and the validation set to estimate the value of the deterministic policy corresponding to the reward model you trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c: tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the validation set and the evaluate_sniw function defined above, you can tune the hyperparameters of the training loop including the number of steps, batch_size, and learning rate. Try some different settings and report your results. (Hint: try training for longer) \n",
    "\n",
    "Then after tuning on the validation set, report the value on the test set using the evaluate_full_feedback function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d: modifying the objective (importance weighting)\n",
    "\n",
    "Now try modifying the loss function to importance weight the loss towards the uniform policy. Train the policy and print the learning curve and test value. Explain the performance relative to the standard direct method without importance weighting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dm_iw_loss(preds, actions, rewards, propensities, hyperparams):\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: importance weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the policy model\n",
    "\n",
    "Now we need to define our policy model class. We will use a simple log-linear model in pytorch with a predict_proba method like an SKLearn model. Note that this is exactly the \"multinominal logistic regression policy\" we referred to in lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPolicyModel(nn.Module):\n",
    "    def __init__(self, context_dim, n_actions):\n",
    "        super(LinearPolicyModel, self).__init__()\n",
    "        self.linear = nn.Linear(context_dim, n_actions)\n",
    "        self.output_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        logits = self.linear(X)\n",
    "        pi_probs = self.output_layer(logits)\n",
    "        return pi_probs\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        return self.forward(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a: defining the learning objective\n",
    "\n",
    "To learn a policy with pytorch, we need to define a loss function to minimize. \n",
    "\n",
    "For this question, please define the loss function for the importance weighted method. The function takes in a batch (pi_probs, actions, rewards, propensities) and returns a scalar tensor for the loss on this batch.  Note that pi_probs is the probability assigned by the policy to each action. Once the function is defined, print out the loss on a batch of data.\n",
    "\n",
    "NOTE: because pytorch does minimization by default, we need to take our standard objective to maximize value and multiply it by -1 so that when pytorch tries to minimize the loss it will maximize the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iw_loss(pi_probs, actions, rewards, propensities, hyperparams):\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b: learning a policy\n",
    "\n",
    "Now we will learn a policy using our loss. Using the same training loop as before, train the policy, plot the learning curve, tune the hyperparameters using the val set, and report the value on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part c: modifying the objective (clipping)\n",
    "\n",
    "Now we will modify the objective to reduce variance. First consider the simplest approach, clipping the weights. Define a loss that uses clipped weights and then train a policy, plot the learning curve, tune the hyperparameters using the val set and then report the test value. To do this, use the hyperparams dict to pass the clipping threshold to the loss function. \n",
    "\n",
    "NOTE: To make the clipping amenable to gradient-based optimization, we need to be sure not to clip the probabilities coming out of our learned policy (this will zero out the gradients and cut off the learning signal). Instead, be sure to only clip the inverse propensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clipped_iw_loss(pi_probs, actions, rewards, propensities, hyperparams):\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part d: modifying the objective (POEM)\n",
    "\n",
    "Now we will consider a further modification by introducing the empirical variance as a regularizer to get the POEM algorithm (https://arxiv.org/abs/1502.02362). Define the POEM loss function and then train a model, plot the learning curve, tune the hyperparameters using the val set, and report the test value. Note that now you have two hyperparameters: the clipping threshold M and the regularization weight lambda. \n",
    "\n",
    "NOTE: to make POEM amenable to SGD, we need to make an approximation of the variance term over the batch instead of over the full dataset. For large batch_size, this approximation will be reasonable. Explicitly, when computing the objective for one batch we will only use the datapoints in that batch to compute an estimate of the empirical variance (instead of using the whole dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poem_loss(pi_probs, actions, rewards, propensities, hyperparams):\n",
    "    raise NotImplemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part e: analysis\n",
    "\n",
    "Please compare and contrast the results for the 3 variants of importance weighting. Then compare and contrast the results with those of the DM. What factors about this particular dataset and choice of model might explain these results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: using neural models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPPolicyModel(nn.Module):\n",
    "    def __init__(self, context_dim, n_actions, hidden_dim = 100):\n",
    "        super(MLPPolicyModel, self).__init__()\n",
    "        self.l1 = nn.Linear(context_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, num_actions)\n",
    "        self.output_layer = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = self.l1(X)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        logits = self.l3(x)\n",
    "        pi_probs = self.output_layer(logits)\n",
    "        return pi_probs\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        return self.forward(X).detach().numpy()\n",
    "    \n",
    "    \n",
    "class MLPRewardModel(nn.Module):\n",
    "    def __init__(self, context_dim, n_actions,  hidden_dim = 100):\n",
    "        super(MLPRewardModel, self).__init__()\n",
    "        self.l1 = nn.Linear(context_dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.l3 = nn.Linear(hidden_dim, num_actions)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x = self.l1(X)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        preds = self.l3(x)\n",
    "        return preds\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        return self.forward(X).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a: Compare DM to POEM\n",
    "\n",
    "Train policies using the direct method and POEM, but now using the neural models. Plot the learning curves and report the test value.\n",
    "\n",
    "NOTE: Try training with varying n_steps. What happens when you train for too long? Not long enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b: analysis\n",
    "\n",
    "Please compare the results using the neural models to those from before with the linear models. What is different? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: trying different datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part a: dataset size\n",
    "\n",
    "To examing the effect of dataset size on the performance of the various algorithms, we will sweep over a few different sizes. Using the load_data function, sweep over datasets of sizes (1000, 2000, 5000, 10000). For each dataset, train the DM and POEM policies using the neural modeels. Plot the dataset size against the test value of each of the learned policies for the various methods. Then explain what is going on in the plot. Why might dataset size have a different impact on the different algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part b: logging policy\n",
    "\n",
    "Now do the same as in part a (sweeping over dataset size and producing a plot), but generate the datasets with log_type = 'uniform' so that the data is generated by a uniform policy. How do the results differ from before? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
